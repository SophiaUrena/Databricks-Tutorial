{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3bd773-8006-45e4-a9bf-cfcaf3c0baa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔄 Incremental Data Ingestion in Databricks\n",
    "\n",
    "Incremental data ingestion enables efficient loading of only **new or updated files**, avoiding reprocessing of previously handled data. This is essential for scalable and cost-effective data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 📥 COPY INTO Command\n",
    "\n",
    "**Definition:**  \n",
    "The `COPY INTO` command is a SQL-based method used to load data from a specified file location into a Delta table.\n",
    "\n",
    "### 🔹 Key Characteristics:\n",
    "- **Idempotent** – Skips files that have already been processed.\n",
    "- **Incremental** – Loads only new files, ideal for growing datasets.\n",
    "- **Flexible** – Supports various formats (CSV, Parquet, etc.) and schema evolution options.\n",
    "\n",
    "### 🔹 Requirements:\n",
    "- Target Delta table\n",
    "- Source file path\n",
    "- File format\n",
    "- Optional settings (e.g., header, schema evolution)\n",
    "\n",
    "This method is simple and efficient for small to medium-scale file ingestion workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Auto Loader\n",
    "\n",
    "**Definition:**  \n",
    "Auto Loader uses **Structured Streaming** to continuously ingest new files from cloud storage. It is designed for **high-volume, scalable** data pipelines.\n",
    "\n",
    "### 🔹 Benefits:\n",
    "- Can handle **millions of files per hour**\n",
    "- Tracks progress via **checkpointing** to ensure **exactly-once** file processing\n",
    "- Supports **schema inference** and **schema evolution**\n",
    "\n",
    "### 🔹 How It Works:\n",
    "- Uses `readStream` with format `\"cloudFiles\"`\n",
    "- Monitors directories for new files and automatically queues them for ingestion\n",
    "- Works with multiple formats like CSV, JSON, and Parquet\n",
    "\n",
    "### 🔹 Schema Management:\n",
    "- Automatically infers schema of incoming data\n",
    "- Can optionally **store inferred schema** to improve startup time and consistency\n",
    "\n",
    "---\n",
    "\n",
    "## 🤔 Choosing Between COPY INTO vs. Auto Loader\n",
    "\n",
    "| Method        | Best For                              | Key Advantages                  |\n",
    "|---------------|----------------------------------------|----------------------------------|\n",
    "| **COPY INTO** | Small to medium workloads (thousands of files) | Simpler setup, SQL-based         |\n",
    "| **Auto Loader** | Large-scale ingestion (millions of files)     | Scalable, streaming, fault-tolerant |\n",
    "\n",
    "> Auto Loader is generally recommended for cloud-based ingestion at scale due to its performance and reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Practical Application\n",
    "\n",
    "The session concludes by transitioning into hands-on practice, where you'll apply these techniques — especially **Auto Loader** — to set up reliable and scalable ingestion pipelines within Databricks.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2  - Incremental Data Integration",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
