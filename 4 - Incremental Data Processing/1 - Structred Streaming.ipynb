{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f470c21-0103-4de9-a0f2-bbdf2d0f50fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 🔄 Structured Streaming in Spark\n",
    "\n",
    "## 📡 Understanding Data Streams\n",
    "Data streams represent continuously growing data sources such as:\n",
    "- New JSON log files landing in cloud storage\n",
    "- Change Data Capture (CDC) from databases\n",
    "- Event streams from systems like Kafka\n",
    "  - Evemts queued in a pub/sub messaging feed\n",
    "\n",
    "Instead of processing the entire dataset repeatedly, modern streaming systems focus on **incremental processing**, handling only **new data** since the last update.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Processing Data Stream\n",
    "2 approaches:\n",
    "1. Reprocess the entire source dtaaset each time\n",
    "2. Only process those new data added since last update\n",
    "    - Structred Streaming\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Spark Structured Streaming Overview\n",
    "Spark Structured Streaming is a powerful engine that treats data streams as **unbounded tables**.  \n",
    "You write queries on streaming data just like static tables — new records simply appear as new rows.\n",
    "\n",
    "It allows you to query an infinite data source where automatically detects new data nad persists the rewsults incremetally into a data sink\n",
    "\n",
    "A **sink** is a durable file systm, such as files or tables\n",
    "\n",
    "---\n",
    "\n",
    "## 💧 Delta Lake Integration\n",
    "Delta Lake integrates seamlessly with Structured Streaming, allowing for efficient real-time ingestion and query capabilities.\n",
    "\n",
    "- Use `spark.readStream()` to read from Delta tables.\n",
    "- Write streaming results using `dataframe.writeStream()` with various configurations.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Configuring Streaming Writes\n",
    "\n",
    "Key configurations for writing data streams include:\n",
    "\n",
    "- **Trigger Intervals** – Controls how often Spark processes new data.\n",
    "- **Checkpointing** – Ensures fault tolerance by tracking the state and progress of a stream, allowing recovery in case of failure.\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ Trigger Options\n",
    "\n",
    "- **Micro-Batch Mode** – Processes data in small time-based intervals.\n",
    "- **One-Time Batch Mode** – Executes a single batch to process available data.\n",
    "Each mode suits different latency and throughput requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 Output Modes\n",
    "\n",
    "- **Append Mode** – Writes only newly arrived rows.\n",
    "- **Complete Mode** – Rewrites the full result every time data is processed.\n",
    "\n",
    "Choice of mode depends on the use case and the type of transformation being applied.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Fault Tolerance & Guarantees\n",
    "\n",
    "Structured Streaming offers:\n",
    "1. Fault Tolerance\n",
    "   - Checkpointing + Write-ahead logs\n",
    "     - Record the offset range of data being processed during each trigger interval\n",
    "2. Exactly-once guarnatee\n",
    "   - Idempotent sinks\n",
    "     - multiple writes of the same data, of course identified by the offset, do not result in duplicates being written to the sink\n",
    "\n",
    "Overall:\n",
    "- **Exactly-once processing semantics** – Prevents duplicates or data loss.\n",
    "- **Checkpoints & Write-Ahead Logs** – Ensures resilience and recoverability of streaming jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Operations on Streaming DataFrames\n",
    "\n",
    "Most transformations supported in static DataFrames are also supported in streaming:\n",
    "Some operations are not supported by streaming DataFrame\n",
    " - Sorting\n",
    " - Deduplication\n",
    "\n",
    "⚠️ Some operations like **global sorting** and **deduplication** are **not supported** due to their complexity in unbounded contexts.\n",
    "\n",
    "---\n",
    "\n",
    "## ⏳ Advanced Streaming Techniques\n",
    "\n",
    "There are advanced streaming methods that can help acheive operations that are not supported.\n",
    "- **Windowing** – Groups data into defined time windows for time-based analysis.\n",
    "- **Watermarking** – Handles **late-arriving data**, helping avoid processing outdated records indefinitely.\n",
    "\n",
    "---\n",
    "\n",
    "Structured Streaming enables **real-time analytics** on live data while offering the flexibility and expressiveness of traditional Spark SQL — making it ideal for modern, scalable, and fault-tolerant data pipelines.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "1 - Structred Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
