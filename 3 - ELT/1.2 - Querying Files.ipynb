{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0511668-a34d-49e0-95ea-ff9738159e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/derar-alhussein/Databricks-Certified-Data-Engineer-Associate/main/Includes/images/bookstore_schema.png\" alt=\"Databricks Learning\" style=\"width: 600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1545f54a-783e-428b-b869-3965da02e8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying JSON  \n",
    "\n",
    "JSON is a self-describing format, meaning it includes schema information within the file itself.  \n",
    "This makes it ideal for querying directly with Spark SQL, as the schema can be automatically inferred.  \n",
    "\n",
    "- Use backticks (`) around file paths to avoid syntax errors.\n",
    "- JSON files are well-suited for structured, semi-structured, and nested data.\n",
    "- You can also query multiple JSON files using wildcard characters.\n",
    "- The `input_file_name()` function helps track which file each record came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9d1bfe3-8a2d-49cc-81d9-1952e3780642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Copy-Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe19b109-1edf-4a57-9d2d-7a09b95c593a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--This will show all the files related here\n",
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_bookstore}/customers-json\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d32931-0545-47bf-8c09-de29b40c2e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--This would query only the specified file\n",
    "SELECT * FROM json.`${dataset.bookstore}/customers-json/export_001.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487a53f7-152e-4a20-a5d4-6ce6db08ad47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--This would query all of the JSON files starting with the name export_ \n",
    "SELECT * FROM json.`${dataset.bookstore}/customers-json/export_*.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab217bdf-6400-46c9-b8b5-6ab92623fda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Query the complete directory of files, assuming all the files in the directory have the same format and schema\n",
    "SELECT * FROM json.`${dataset.bookstore}/customers-json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e3987b-822e-4d6b-bf2a-1479a53d24d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT count(*) FROM json.`${dataset.bookstore}/customers-json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6878d42f-6ab8-4bd1-8df0-fc8baba17b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--When reading multiple files, it's useful to add the input_file_name function, which is a built-in Spark SQL command that records the source data file for each record.\n",
    "SELECT *,\n",
    "    input_file_name() source_file\n",
    "  FROM json.`${dataset.bookstore}/customers-json`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b05138a-7435-45f2-a8be-c9601c9906da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying text Format\n",
    "\n",
    "The `text` format is used to read raw text-based files such as `.txt`, `.tsv`, `.csv`, or even malformed JSON.  \n",
    "Each row is loaded as a single column named `value`.\n",
    "\n",
    "- Useful for inspecting raw data or handling corrupted input files.\n",
    "- It’s ideal when you don’t know the structure of the data in advance.\n",
    "- You can apply parsing and transformations manually after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27262035-ad40-4b17-9f39-bddcd2300c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM text.`${dataset.bookstore}/customers-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "390fc969-6d5c-4547-8cd8-9b348baa1551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying binaryFile Format\n",
    "\n",
    "The `binaryFile` format is used for reading binary content like images, PDFs, or other non-text files.\n",
    "\n",
    "- Each row includes metadata like file path, size, and binary content.\n",
    "- Often used in machine learning or document/image processing workflows.\n",
    "- Common file types include `.png`, `.jpg`, `.pdf`, etc.\n",
    "- Unlike structured formats, binary files do not support schema inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e3d6028-d0cf-4985-b365-fd201ac06bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM binaryFile.`${dataset.bookstore}/customers-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f94b99a7-5162-4a1a-895c-0582bd5c5f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Querying CSV \n",
    "\n",
    "CSV is a simple, row-based format that **does not include schema information**.\n",
    "\n",
    "- Databricks must infer the schema or have it explicitly defined.\n",
    "- Header rows may be misread as data if not handled properly.\n",
    "- Options like `header=true`, `inferSchema=true`, and `delimiter=','` are often required.\n",
    "- Use caution: inconsistent formatting or missing headers can lead to incorrect parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe21277-fa84-4f8d-afa2-8b36d65e9b6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM csv.`${dataset.bookstore}/books-csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9ba073-a467-48e6-b65a-f182938121bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--The USING keyword allows us to create a table against an external source like CSV format\n",
    "--Here we need to specify the table schema (column names, types, file format, header, delimiter, etc)\n",
    "CREATE TABLE books_csv\n",
    "  (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  header = \"true\",\n",
    "  delimiter = \";\"\n",
    ")\n",
    "LOCATION \"${dataset.bookstore}/books-csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2721c307-b730-4418-93a4-9013c927547b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM books_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c482d1f5-28a0-4ff8-8850-5f4b149f4c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Limitations of Non-Delta Tables\n",
    "\n",
    "External tables created from formats like CSV, JSON, or Parquet without Delta:\n",
    "\n",
    "- Lack ACID compliance.\n",
    "- Don’t support features like schema enforcement, time travel, or optimized file compaction.\n",
    "- Are not automatically updated when new files are added unless the table is refreshed.\n",
    "- Have limited support for concurrent writes and reads.\n",
    "\n",
    "To overcome these issues, convert external tables into **Delta tables** using CTAS or data loading techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd91b76e-9a6f-4bdb-a9a4-f1a06b9160a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Shows detail on the table\n",
    "DESCRIBE EXTENDED books_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f081b12-ac89-4cce-aa92-9254c636ee5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Check how many CSV files are in the directory\n",
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_bookstore}/books-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41fc304-787d-4284-8a9c-ed0c57de1952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Use a Spark DataFrame API that allows us to write data in a specific format like CSV\n",
    "%python\n",
    "(spark.read\n",
    "        .table(\"books_csv\")\n",
    "      .write\n",
    "        .mode(\"append\")\n",
    "        .format(\"csv\")\n",
    "        .option('header', 'true')\n",
    "        .option('delimiter', ';')\n",
    "        .save(f\"{dataset_bookstore}/books-csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f67293-fbed-4ebf-b426-85470644cffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--View how many CSV files are in the directory\n",
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_bookstore}/books-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a42562f1-a18a-4851-b432-7d9532a8d2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Even with new data being successfully written to the table directory, we are unable to see this new data\n",
    "--This is because Spark automatically cached the underlying data in local storage to ensure that on subsequent queries, Spark will provide the optimal performance by querying this local cache\n",
    "SELECT COUNT(*) FROM books_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6f3266-3443-42d3-b757-d856604d737a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--Manually refresh the cache of the data by running the REFRESH TABLE command\n",
    "REFRESH TABLE books_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d459155-0ee8-49d1-8f8b-3344b69c49b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM books_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350bbf0e-ff43-4cfb-ae7b-28ee260245d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CTAS Statements (Create Table As Select)\n",
    "\n",
    "CTAS (Create Table As Select) allows you to:\n",
    "\n",
    "- Create a new table and populate it with data in a single step.\n",
    "- Automatically infer the schema from the result of the SELECT statement.\n",
    "- Transform and clean data while loading into a Delta Lake table.\n",
    "- Improve performance and data reliability by leveraging Delta features.\n",
    "\n",
    "It’s commonly used to convert external tables or temporary views into optimized Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4bfaaf-b329-4c38-9e14-4b2ceb11963f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--CTAS files do not support specifying additional file options\n",
    "CREATE TABLE customers AS\n",
    "SELECT * FROM json.`${dataset.bookstore}/customers-json`;\n",
    "\n",
    "DESCRIBE EXTENDED customers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbeb9f70-37ee-4609-b679-53f00bc6231f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE books_unparsed AS\n",
    "SELECT * FROM csv.`${dataset.bookstore}/books-csv`;\n",
    "\n",
    "SELECT * FROM books_unparsed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c21b808-3715-47cb-9d18-1e0d07f524eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TEMP VIEW books_tmp_vw\n",
    "   (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path = \"${dataset.bookstore}/books-csv/export_*.csv\",\n",
    "  header = \"true\",\n",
    "  delimiter = \";\"\n",
    ");\n",
    "\n",
    "CREATE TABLE books AS\n",
    "  SELECT * FROM books_tmp_vw;\n",
    "  \n",
    "SELECT * FROM books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "760822aa-0385-42ec-9ca4-8701d1c73e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED books"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.2 - Querying Files",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
